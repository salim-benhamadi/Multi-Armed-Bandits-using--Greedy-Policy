{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/salimhammadi07/multi-armed-bandits-using-greedy-policy?scriptVersionId=129532730\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# I. Understanding the problem","metadata":{}},{"cell_type":"markdown","source":"In probability theory and machine learning, the multi-armed bandit problem (sometimes called the K-[1] or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.\n\n![](https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg)\n\nSource  : https://en.wikipedia.org/wiki/Multi-armed_bandit","metadata":{}},{"cell_type":"markdown","source":"# II. Importing dependicies ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport seaborn as sns\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('fivethirtyeight') \nnp.random.seed(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-05T06:08:18.770321Z","iopub.execute_input":"2022-10-05T06:08:18.771528Z","iopub.status.idle":"2022-10-05T06:08:18.780245Z","shell.execute_reply.started":"2022-10-05T06:08:18.771474Z","shell.execute_reply":"2022-10-05T06:08:18.77878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# III. Setting up the envirement ","metadata":{}},{"cell_type":"code","source":"class Environment:\n\n    def __init__(self, nbr_bandits, probs = []):\n        self.probs = probs #success probabilities for each bandit\n        self.nbr_bandits = nbr_bandits\n           \n            \n    def step(self, action):\n        # Pull arm and get stochastic reward (1 for success, 0 for failure)\n            return 1 if (np.random.random()  < self.probs[action]) else 0\n    \n    def brandits(self):\n        # print the brandits probabilites \n        for i in range(self.nbr_bandits):\n            print (\"Bandit #{} = {}% success rate \".format(i,self.probs[i] *100))","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.783345Z","iopub.execute_input":"2022-10-05T06:08:18.783908Z","iopub.status.idle":"2022-10-05T06:08:18.800177Z","shell.execute_reply.started":"2022-10-05T06:08:18.783856Z","shell.execute_reply":"2022-10-05T06:08:18.798842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nbr_bandits = 10\nprobs = []\nfor i in range(nbr_bandits):\n    probs.append(random.random()) \n    \nenv = Environment(nbr_bandits, probs) ","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.801644Z","iopub.execute_input":"2022-10-05T06:08:18.802676Z","iopub.status.idle":"2022-10-05T06:08:18.815322Z","shell.execute_reply.started":"2022-10-05T06:08:18.802638Z","shell.execute_reply":"2022-10-05T06:08:18.814117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # III. Setting up the Agent","metadata":{}},{"cell_type":"code","source":"class Agent:\n    def __init__(self, nbr_action, epsilon = 0.1):\n        self.nbr_action = nbr_action # number of total actions the agent is allowed to do \n        self.epsilon = epsilon # Represent the threshold, Based on it the Agent will know how much it can exploit per round and how much it can explore\n        self.n = [0]*nbr_action # The number each action the agent did\n        self.Q = [0]*nbr_action # value Q(a)\n        \n    def Q_value(self, action, reward):\n        # Update Q action-value given (action, reward)\n        self.n[action] += 1\n        self.Q[action] += (1.0/self.n[action]) * (reward - self.Q[action])\n        \n    def act(self):\n        # Epsilon-greedy policy\n        \n        # explore\n        if np.random.random() < self.epsilon: \n            return np.random.randint(self.nbr_action)\n        # exploit\n        else: \n            return self.Q.index(max(self.Q))","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.817952Z","iopub.execute_input":"2022-10-05T06:08:18.818348Z","iopub.status.idle":"2022-10-05T06:08:18.827514Z","shell.execute_reply.started":"2022-10-05T06:08:18.818317Z","shell.execute_reply":"2022-10-05T06:08:18.826358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VI. Expermerit","metadata":{}},{"cell_type":"markdown","source":"## VI.1 ℇ-Greedy Policy","metadata":{}},{"cell_type":"markdown","source":"### VI.1.1 Implementation","metadata":{}},{"cell_type":"code","source":"episodes = 10000\nagent = Agent(nbr_bandits)  \nactions, rewards = [], []\nfor episode in tqdm(range(episodes)):\n    action = agent.act() # sample policy\n    reward = env.step(action) # take step + get reward\n    agent.Q_value(action, reward) # update Q\n    actions.append(action)\n    rewards.append(reward)\n#     print(\"Running multi-armed bandits with Action = {}, Reward obtained = {}\".format(action, reward))","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.829104Z","iopub.execute_input":"2022-10-05T06:08:18.829459Z","iopub.status.idle":"2022-10-05T06:08:18.886667Z","shell.execute_reply.started":"2022-10-05T06:08:18.829429Z","shell.execute_reply":"2022-10-05T06:08:18.88543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VI.1.2 Evaluation ","metadata":{}},{"cell_type":"code","source":"env.brandits()","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.888458Z","iopub.execute_input":"2022-10-05T06:08:18.889582Z","iopub.status.idle":"2022-10-05T06:08:18.896329Z","shell.execute_reply.started":"2022-10-05T06:08:18.889535Z","shell.execute_reply":"2022-10-05T06:08:18.895114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total rewards earned is {} out 10000'.format(sum(rewards)))","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.898145Z","iopub.execute_input":"2022-10-05T06:08:18.898642Z","iopub.status.idle":"2022-10-05T06:08:18.913263Z","shell.execute_reply.started":"2022-10-05T06:08:18.898597Z","shell.execute_reply":"2022-10-05T06:08:18.911733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 10))\n\nax = pd.DataFrame(actions).value_counts().plot(kind='barh')\nfor index, value in enumerate(pd.DataFrame(actions).value_counts()):\n    ax.text(value, index, str(value))","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:18.915001Z","iopub.execute_input":"2022-10-05T06:08:18.915516Z","iopub.status.idle":"2022-10-05T06:08:19.261885Z","shell.execute_reply.started":"2022-10-05T06:08:18.915468Z","shell.execute_reply":"2022-10-05T06:08:19.260486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.DataFrame({\"The Estimated Rewards Qt(a)\" : agent.Q,\n                   \"The Expected Rewards Q*(a)\" : probs})\ndf.plot(kind=\"bar\",figsize = (20,10))\nplt.title(\"Comparing the The Estimated Rewards to Expexted Rewards\")\nplt.xlabel(\"Bandits\")\nplt.ylabel(\"Brandits probabilites \")","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:19.263816Z","iopub.execute_input":"2022-10-05T06:08:19.264279Z","iopub.status.idle":"2022-10-05T06:08:19.631218Z","shell.execute_reply.started":"2022-10-05T06:08:19.264236Z","shell.execute_reply":"2022-10-05T06:08:19.630228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_rewards = []\nfor i in range(episodes):\n    if i == 0 :\n        acc_rewards.append(0)\n    else:\n        acc_rewards.append(sum(rewards[:i])/i)\n\ndf = pd.DataFrame({\"steps\" : range(episodes) ,\n                   \"Average Rewards\" : acc_rewards})\ndf.plot(x = 'steps',y ='Average Rewards',figsize = (20,10))\nplt.title(\"Average Rewards per step\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Average Rewards\")","metadata":{"execution":{"iopub.status.busy":"2022-10-05T06:08:19.633612Z","iopub.execute_input":"2022-10-05T06:08:19.63404Z","iopub.status.idle":"2022-10-05T06:08:20.678157Z","shell.execute_reply.started":"2022-10-05T06:08:19.634009Z","shell.execute_reply":"2022-10-05T06:08:20.67724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_rewards = []\nfor i in range(episodes):\n    if i == 0 :\n        acc_rewards.append(0)\n    else:\n        acc_rewards.append(sum(actions[:i])/i)\n\ndf = pd.DataFrame({\"steps\" : range(episodes) ,\n                   \"% Optimal Action\" : acc_rewards})\ndf.plot(x = 'steps',y ='% Optimal Action',figsize = (20,10))\nplt.title(\"% Optimal Action per step\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"% Optimal Action\")","metadata":{"execution":{"iopub.status.busy":"2023-01-10T15:49:56.245898Z","iopub.execute_input":"2023-01-10T15:49:56.247561Z","iopub.status.idle":"2023-01-10T15:49:56.362456Z","shell.execute_reply.started":"2023-01-10T15:49:56.247435Z","shell.execute_reply":"2023-01-10T15:49:56.360084Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/716460940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0macc_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'episodes' is not defined"],"ename":"NameError","evalue":"name 'episodes' is not defined","output_type":"error"}]}]}